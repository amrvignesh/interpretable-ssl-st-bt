{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ---\n",
        "# jupyter:\n",
        "#   jupytext:\n",
        "#     text_representation:\n",
        "#       extension: .py\n",
        "#       format_name: light\n",
        "#       format_version: '1.5'\n",
        "#       jupytext_version: 1.14.5 # Or your version\n",
        "#   kernelspec:\n",
        "#     display_name: Python 3 (ipykernel) # Or your kernel name\n",
        "#     language: python\n",
        "#     name: python3\n",
        "# ---\n",
        "\n",
        "# # BraTS 2023 Semi-Supervised SwinUNETR Demo & Explainability\n",
        "#\n",
        "# This notebook demonstrates the results of the trained SSL SwinUNETR model,\n",
        "# visualizes segmentations, and shows attention maps using Attention Rollout.\n",
        "\n",
        "# ## 1. Setup and Imports\n",
        "# Import necessary libraries and configure paths.\n",
        "\n",
        "# +\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "from functools import partial\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import torch\n",
        "from monai.config import print_config\n",
        "from monai.data import Dataset, DataLoader, decollate_batch, list_data_collate\n",
        "from monai.inferers import sliding_window_inference\n",
        "from monai.networks.nets import SwinUNETR\n",
        "from monai import transforms\n",
        "from monai.transforms import (\n",
        "    AsDiscrete,\n",
        "    Activations,\n",
        "    Compose,\n",
        "    LoadImaged,\n",
        "    EnsureChannelFirstd,\n",
        "    EnsureTyped,\n",
        "    NormalizeIntensityd,\n",
        "    # Add other necessary transforms used during validation/inference\n",
        ")\n",
        "from monai.visualize import plot_2d_or_3d_image # For visualization\n",
        "\n",
        "# Add project root to path if necessary (if running notebook from a subfolder)\n",
        "# module_path = os.path.abspath(os.path.join('..'))\n",
        "# if module_path not in sys.path:\n",
        "#     sys.path.append(module_path)\n",
        "\n",
        "# Import helper functions if defined in separate .py files\n",
        "# from brats_ssl_train import get_brats2023_datalists # Example\n",
        "\n",
        "print_config()\n",
        "# -\n",
        "\n",
        "# ## 2. Configuration\n",
        "# Define paths to data, model checkpoint, and other parameters.\n",
        "\n",
        "# +\n",
        "# --- Parameters to Set ---\n",
        "DATA_DIR = './data/brats2023' # Path to dataset\n",
        "MAPPING_FILE = 'BraTS2023_2017_GLI_Mapping.xlsx - Sheet1.csv' # Mapping file name\n",
        "MODEL_CHECKPOINT = './output_brats_ssl/model_best.pt' # Path to the trained model weights\n",
        "OUTPUT_DIR = './output_brats_ssl/notebook_viz' # Directory to save visualizations\n",
        "\n",
        "ROI_SIZE = (128, 128, 128) # Must match training ROI\n",
        "IN_CHANNELS = 4 # t1c, t1n, t2f, t2w\n",
        "OUT_CHANNELS = 3 # ET, TC, WT\n",
        "FEATURE_SIZE = 48 # Must match trained model\n",
        "\n",
        "# Select a case ID for demonstration (e.g., from your validation set)\n",
        "# Replace with an actual patient ID from the 'BraTS 2023' column of your mapping CSV\n",
        "CASE_ID = \"BraTS-GLI-00000-000\" # !!! REPLACE WITH A VALID CASE !!!\n",
        "# --- End Parameters ---\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "# -\n",
        "\n",
        "# ## 3. Load Model\n",
        "# Load the trained SwinUNETR model from the checkpoint file.\n",
        "\n",
        "# +\n",
        "print(f\"Loading model checkpoint from: {MODEL_CHECKPOINT}\")\n",
        "if not os.path.exists(MODEL_CHECKPOINT):\n",
        "    raise FileNotFoundError(f\"Model checkpoint not found at {MODEL_CHECKPOINT}\")\n",
        "\n",
        "model = SwinUNETR(\n",
        "    img_size=ROI_SIZE,\n",
        "    in_channels=IN_CHANNELS,\n",
        "    out_channels=OUT_CHANNELS,\n",
        "    feature_size=FEATURE_SIZE,\n",
        "    use_checkpoint=False, # No need for checkpointing during inference usually\n",
        ").to(device)\n",
        "\n",
        "checkpoint = torch.load(MODEL_CHECKPOINT, map_location=device)\n",
        "# Load state dict - handle potential DataParallel wrapping if model was saved that way\n",
        "if 'state_dict' in checkpoint:\n",
        "    model_state_dict = checkpoint['state_dict']\n",
        "    # Remove 'module.' prefix if saved using DataParallel\n",
        "    if list(model_state_dict.keys())[0].startswith('module.'):\n",
        "        model_state_dict = {k[len(\"module.\"):]: v for k, v in model_state_dict.items()}\n",
        "    model.load_state_dict(model_state_dict)\n",
        "    print(f\"Loaded model state_dict from epoch {checkpoint.get('epoch', 'N/A')}\")\n",
        "else:\n",
        "    # If the checkpoint only contains the state_dict\n",
        "    model.load_state_dict(checkpoint)\n",
        "    print(\"Loaded model state_dict directly.\")\n",
        "\n",
        "model.eval() # Set model to evaluation mode\n",
        "print(\"Model loaded successfully.\")\n",
        "# -\n",
        "\n",
        "# ## 4. Prepare Data for Selected Case\n",
        "# Load the MRI scans and ground truth label for the chosen `CASE_ID`.\n",
        "\n",
        "# +\n",
        "# Construct file paths for the selected case\n",
        "# Adjust 'training_data_root' if needed\n",
        "training_data_root = os.path.join(DATA_DIR, \"ASNR-MICCAI-BraTS2023-GLI-Challenge-TrainingData\")\n",
        "patient_path = os.path.join(training_data_root, CASE_ID)\n",
        "\n",
        "if not os.path.isdir(patient_path):\n",
        "     raise FileNotFoundError(f\"Directory for case {CASE_ID} not found at {patient_path}\")\n",
        "\n",
        "image_files = [\n",
        "    os.path.join(patient_path, f\"{CASE_ID}-t1c.nii.gz\"),\n",
        "    os.path.join(patient_path, f\"{CASE_ID}-t1n.nii.gz\"),\n",
        "    os.path.join(patient_path, f\"{CASE_ID}-t2f.nii.gz\"), # FLAIR\n",
        "    os.path.join(patient_path, f\"{CASE_ID}-t2w.nii.gz\"), # T2\n",
        "]\n",
        "label_file = os.path.join(patient_path, f\"{CASE_ID}-seg.nii.gz\")\n",
        "\n",
        "# Check if files exist\n",
        "missing_files = [f for f in image_files + [label_file] if not os.path.exists(f)]\n",
        "if missing_files:\n",
        "    raise FileNotFoundError(f\"Missing files for case {CASE_ID}: {missing_files}\")\n",
        "\n",
        "case_data = [{\"image\": image_files, \"label\": label_file}]\n",
        "\n",
        "# Define transforms for loading and preprocessing (similar to validation)\n",
        "# Ensure these match the preprocessing the model expects!\n",
        "inference_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"image\", \"label\"], image_only=False),\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "        transforms.ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"), # Convert label for comparison\n",
        "        EnsureTyped(keys=[\"image\", \"label\"], dtype=torch.float32),\n",
        "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
        "        # Add Spacingd or Orientationd if they were used during training/validation\n",
        "        # transforms.Spacingd(keys=[\"image\", \"label\"], pixdim=(1.0, 1.0, 1.0), mode=(\"bilinear\", \"nearest\")),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply transforms\n",
        "transformed_data = inference_transforms(case_data[0])\n",
        "image_tensor = transformed_data[\"image\"].unsqueeze(0).to(device) # Add batch dim and move to device\n",
        "label_tensor = transformed_data[\"label\"].unsqueeze(0).to(device) # Add batch dim\n",
        "\n",
        "print(f\"Input image shape: {image_tensor.shape}\") # Should be (1, 4, H, W, D)\n",
        "print(f\"Label shape: {label_tensor.shape}\")     # Should be (1, 3, H, W, D)\n",
        "# -\n",
        "\n",
        "# ## 5. Perform Inference\n",
        "# Run the loaded model on the prepared data using sliding window inference.\n",
        "\n",
        "# +\n",
        "# Define the inferer (same as validation)\n",
        "model_inferer = partial(\n",
        "    sliding_window_inference,\n",
        "    roi_size=ROI_SIZE,\n",
        "    sw_batch_size=1, # Can increase if GPU memory allows\n",
        "    predictor=model,\n",
        "    overlap=0.6, # Use a higher overlap for potentially smoother results\n",
        "    mode=\"gaussian\",\n",
        "    progress=True\n",
        ")\n",
        "\n",
        "# Define post-processing (sigmoid + threshold)\n",
        "post_pred_viz = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
        "\n",
        "# Run inference\n",
        "print(\"Running inference...\")\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    logits = model_inferer(image_tensor)\n",
        "    prediction = post_pred_viz(logits) # Get discrete prediction (0/1)\n",
        "end_time = time.time()\n",
        "print(f\"Inference complete. Time taken: {end_time - start_time:.2f} seconds\")\n",
        "print(f\"Prediction shape: {prediction.shape}\") # Should be (1, 3, H, W, D)\n",
        "\n",
        "# Move prediction to CPU for visualization\n",
        "prediction_cpu = prediction.squeeze(0).cpu().numpy() # Shape (3, H, W, D)\n",
        "label_cpu = label_tensor.squeeze(0).cpu().numpy()     # Shape (3, H, W, D)\n",
        "image_cpu = image_tensor.squeeze(0).cpu().numpy()     # Shape (4, H, W, D)\n",
        "# -\n",
        "\n",
        "# ## 6. Visualize Segmentation Results\n",
        "# Display slices of the input (e.g., FLAIR), ground truth label, and model prediction.\n",
        "\n",
        "# +\n",
        "# Choose a slice number for visualization (axial view)\n",
        "slice_idx = prediction_cpu.shape[3] // 2 # Middle slice\n",
        "# Or choose a slice with significant tumor presence\n",
        "\n",
        "# Select input modality to display (e.g., FLAIR is channel 2: t1c=0, t1n=1, t2f=2, t2w=3)\n",
        "img_display = image_cpu[2, :, :, slice_idx]\n",
        "\n",
        "# Combine multi-channel labels/predictions into a single map for visualization\n",
        "# Class mapping: 1=ET, 2=TC, 3=WT (adjust if needed)\n",
        "# Prediction: Channel 0=ET, 1=TC, 2=WT\n",
        "pred_viz = np.zeros_like(prediction_cpu[0]) # Initialize with background\n",
        "pred_viz[prediction_cpu[2] == 1] = 3 # WT\n",
        "pred_viz[prediction_cpu[1] == 1] = 2 # TC (overwrites WT)\n",
        "pred_viz[prediction_cpu[0] == 1] = 1 # ET (overwrites TC)\n",
        "pred_display = pred_viz[:, :, slice_idx]\n",
        "\n",
        "# Ground Truth: Channel 0=ET, 1=TC, 2=WT\n",
        "gt_viz = np.zeros_like(label_cpu[0])\n",
        "gt_viz[label_cpu[2] == 1] = 3 # WT\n",
        "gt_viz[label_cpu[1] == 1] = 2 # TC\n",
        "gt_viz[label_cpu[0] == 1] = 1 # ET\n",
        "gt_display = gt_viz[:, :, slice_idx]\n",
        "\n",
        "\n",
        "# Plotting\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "plt.suptitle(f\"Segmentation Results for Case: {CASE_ID} (Slice: {slice_idx})\", fontsize=16)\n",
        "\n",
        "axes[0].imshow(img_display, cmap='gray')\n",
        "axes[0].set_title(\"Input FLAIR\")\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(gt_display, cmap='jet', vmin=0, vmax=3) # Use 'jet' or other colormap for labels\n",
        "axes[1].set_title(\"Ground Truth (1:ET, 2:TC, 3:WT)\")\n",
        "axes[1].axis('off')\n",
        "\n",
        "axes[2].imshow(pred_display, cmap='jet', vmin=0, vmax=3)\n",
        "axes[2].set_title(\"Model Prediction (1:ET, 2:TC, 3:WT)\")\n",
        "axes[2].axis('off')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
        "# Save the figure\n",
        "save_path = os.path.join(OUTPUT_DIR, f\"{CASE_ID}_segmentation_slice_{slice_idx}.png\")\n",
        "plt.savefig(save_path)\n",
        "print(f\"Segmentation visualization saved to {save_path}\")\n",
        "plt.show()\n",
        "\n",
        "# Optional: Use MONAI's plot_2d_or_3d_image for more advanced plotting\n",
        "# plot_2d_or_3d_image(image_tensor, slice_idx, figure=fig, subplot=131, title=\"Input\")\n",
        "# plot_2d_or_3d_image(label_tensor, slice_idx, figure=fig, subplot=132, title=\"Ground Truth\")\n",
        "# plot_2d_or_3d_image(prediction, slice_idx, figure=fig, subplot=133, title=\"Prediction\")\n",
        "# plt.show()\n",
        "# -\n",
        "\n",
        "# ## 7. Attention Rollout Visualization\n",
        "#\n",
        "# **--- !!! IMPLEMENTATION REQUIRED !!! ---**\n",
        "#\n",
        "# This section requires implementing the Attention Rollout algorithm specifically for the SwinUNETR architecture used. This typically involves:\n",
        "# 1.  **Hooking into Attention Layers:** Registering forward hooks on the attention layers (specifically the attention matrix computation) within the Swin Transformer blocks of the SwinUNETR model.\n",
        "# 2.  **Extracting Attention Matrices:** During a forward pass (inference) on the input image, capture the attention matrices from each relevant layer/block.\n",
        "# 3.  **Aggregating Attention (Rollout):** Implement the rollout logic. This usually involves matrix multiplying the attention matrices across layers, potentially adding residual connections and layer normalization effects. The exact formula depends on the chosen rollout variant and the Swin Transformer architecture details. Refer to the original Attention Rollout paper [cite: 101] and potentially Swin Transformer specific adaptations.\n",
        "# 4.  **Generating Visualization:** Reshape and visualize the final aggregated attention map, often overlaid on the input image slice.\n",
        "\n",
        "# +\n",
        "# Placeholder function - Replace with your actual implementation\n",
        "def generate_attention_rollout(model, input_tensor, device, **kwargs):\n",
        "    \"\"\"\n",
        "    Placeholder for Attention Rollout implementation for SwinUNETR.\n",
        "\n",
        "    Args:\n",
        "        model: The trained SwinUNETR model.\n",
        "        input_tensor: The input image tensor (B, C, H, W, D).\n",
        "        device: The device (CPU/GPU).\n",
        "        **kwargs: Additional parameters for rollout (e.g., layer selection, head fusion).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The aggregated attention map (e.g., shape H, W, D or H, W for a slice).\n",
        "                    Returns None if not implemented.\n",
        "    \"\"\"\n",
        "    print(\"--- !!! Attention Rollout function not implemented !!! ---\")\n",
        "    # --- Your implementation here ---\n",
        "    # 1. Add hooks to model layers\n",
        "    # 2. Run model(input_tensor)\n",
        "    # 3. Process captured attention matrices using rollout logic\n",
        "    # 4. Remove hooks\n",
        "    # --- End Implementation ---\n",
        "    return None # Return the calculated map\n",
        "\n",
        "# --- Generate and Visualize Attention Map ---\n",
        "print(\"Generating Attention Map (requires implementation)...\")\n",
        "attention_map = generate_attention_rollout(model, image_tensor, device)\n",
        "\n",
        "if attention_map is not None:\n",
        "    # Visualize the attention map (e.g., overlay on the input slice)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.suptitle(f\"Attention Map for Case: {CASE_ID} (Slice: {slice_idx})\", fontsize=16)\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img_display, cmap='gray')\n",
        "    plt.title(\"Input FLAIR\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    # Assuming attention_map is 2D for the slice\n",
        "    attention_display = attention_map # Or attention_map[:, :, slice_idx] if 3D\n",
        "    plt.imshow(img_display, cmap='gray')\n",
        "    plt.imshow(attention_display, cmap='viridis', alpha=0.6) # Overlay attention map\n",
        "    plt.title(\"Attention Rollout Map\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    save_path_attn = os.path.join(OUTPUT_DIR, f\"{CASE_ID}_attention_slice_{slice_idx}.png\")\n",
        "    plt.savefig(save_path_attn)\n",
        "    print(f\"Attention map visualization saved to {save_path_attn}\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipping attention map visualization.\")\n",
        "\n",
        "# -\n",
        "\n",
        "# ## 8. Quantitative Results Summary (Optional)\n",
        "# Load and display key metrics from the training process (e.g., best validation Dice).\n",
        "\n",
        "# +\n",
        "history_path = os.path.join(os.path.dirname(MODEL_CHECKPOINT), \"training_history.json\")\n",
        "\n",
        "if os.path.exists(history_path):\n",
        "    print(f\"Loading training history from: {history_path}\")\n",
        "    with open(history_path, 'r') as f:\n",
        "        history = json.load(f)\n",
        "\n",
        "    best_val_epoch_idx = np.argmax(history['val_mean_dice'])\n",
        "    best_val_dice = history['val_mean_dice'][best_val_epoch_idx]\n",
        "    best_et_dice = history['val_dice_et'][best_val_epoch_idx]\n",
        "    best_tc_dice = history['val_dice_tc'][best_val_epoch_idx]\n",
        "    best_wt_dice = history['val_dice_wt'][best_val_epoch_idx]\n",
        "    # Find the epoch number corresponding to the best validation index\n",
        "    # This assumes val_every logic used during saving history is consistent\n",
        "    val_epochs_count = len(history['val_mean_dice'])\n",
        "    epochs_per_val = args.val_every if 'args' in locals() else 10 # Default if args not available\n",
        "    best_epoch = (best_val_epoch_idx + 1) * epochs_per_val\n",
        "\n",
        "\n",
        "    print(\"\\n--- Best Validation Metrics ---\")\n",
        "    print(f\"Achieved at Epoch: ~{best_epoch}\")\n",
        "    print(f\"Overall Mean Dice: {best_val_dice:.4f}\")\n",
        "    print(f\"  - Enhancing Tumor (ET) Dice: {best_et_dice:.4f}\")\n",
        "    print(f\"  - Tumor Core (TC) Dice:      {best_tc_dice:.4f}\")\n",
        "    print(f\"  - Whole Tumor (WT) Dice:     {best_wt_dice:.4f}\")\n",
        "else:\n",
        "    print(f\"Training history file not found at {history_path}. Cannot display metrics.\")\n",
        "# -\n",
        "\n",
        "# ## 9. Conclusion\n",
        "# Summarize the findings shown in the notebook.\n",
        "#\n",
        "# * The notebook demonstrated loading the SSL-trained SwinUNETR model.\n",
        "# * Inference was performed on a sample BraTS 2023 case.\n",
        "# * Segmentation results were visualized alongside the ground truth.\n",
        "# * (If implemented) Attention Rollout maps provided insight into the model's focus areas.\n",
        "# * Key quantitative metrics from training were summarized."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "imGtxrVaNq7G"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}