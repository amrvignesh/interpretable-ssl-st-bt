{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "082a9e1e-65e3-489e-8554-27f476a8912b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.5.dev2514\n",
      "Numpy version: 2.0.2\n",
      "Pytorch version: 2.6.0+cu124\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: a3ea49fc4e600d131daadad61ea340df25fcfdaa\n",
      "MONAI __file__: /home/users/<username>/dl/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.3.2\n",
      "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "scipy version: 1.12.0\n",
      "Pillow version: 11.1.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "tqdm version: 4.67.1\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.8\n",
      "pandas version: 2.2.3\n",
      "einops version: 0.8.1\n",
      "transformers version: 4.50.2\n",
      "mlflow version: 2.21.2\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n",
      "Using device: cuda\n",
      "Creating datasets and dataloaders (mini-version)...\n",
      "Using structure CSV: /home/users/vraja/dl/data/brats2021challenge/brats_dsc.csv\n",
      "Identifying patient folders within paths containing: ASNR-MICCAI-BraTS2023-GLI-Challenge-TrainingData\n",
      "Found 1251 potential patient directories.\n",
      "--- Using subset of first 10 directories for testing ---\n",
      "Total valid cases processed in subset: 10\n",
      "Using 1 cases as labeled data.\n",
      "Using 9 cases as unlabeled data.\n",
      "Mini Batch size: 1 (Labeled: 0, Unlabeled: 1)\n",
      "Initializing model, loss, optimizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/vraja/dl/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
      "  warn_deprecated(argname, msg, warning_category)\n",
      "/home/users/vraja/dl/monai/utils/deprecate_utils.py:221: FutureWarning: monai.networks.nets.swin_unetr SwinUNETR.__init__:img_size: Argument `img_size` has been deprecated since version 1.3. It will be removed in version 1.5. The img_size argument is not required anymore and checks on the input size are run during forward().\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, Loss, Optimizer Initialized.\n",
      "--- Starting Mini Training Verification (Max Steps: 5) ---\n",
      "Step 1: Unlabeled shapes - Weak: torch.Size([1, 4, 96, 96, 96]), Strong: torch.Size([1, 4, 96, 96, 96])\n",
      "Step 1: Calculated Total Loss: 0.0207\n",
      "Step 1: Backward pass and optimizer step completed.\n",
      "------------------------------\n",
      "Step 2: Unlabeled shapes - Weak: torch.Size([1, 4, 96, 96, 96]), Strong: torch.Size([1, 4, 96, 96, 96])\n",
      "Step 2: Calculated Total Loss: 0.0150\n",
      "Step 2: Backward pass and optimizer step completed.\n",
      "------------------------------\n",
      "Step 3: Unlabeled shapes - Weak: torch.Size([1, 4, 96, 96, 96]), Strong: torch.Size([1, 4, 96, 96, 96])\n",
      "Step 3: Calculated Total Loss: 0.0152\n",
      "Step 3: Backward pass and optimizer step completed.\n",
      "------------------------------\n",
      "Step 4: Unlabeled shapes - Weak: torch.Size([1, 4, 96, 96, 96]), Strong: torch.Size([1, 4, 96, 96, 96])\n",
      "Step 4: Calculated Total Loss: 0.0119\n",
      "Step 4: Backward pass and optimizer step completed.\n",
      "------------------------------\n",
      "Step 5: Unlabeled shapes - Weak: torch.Size([1, 4, 96, 96, 96]), Strong: torch.Size([1, 4, 96, 96, 96])\n",
      "Step 5: Calculated Total Loss: 0.0140\n",
      "Step 5: Backward pass and optimizer step completed.\n",
      "------------------------------\n",
      "--- Mini Training Verification Finished ---\n",
      "Steps Run: 5\n",
      "Avg Total Loss: 0.0154\n",
      "Avg Sup Loss: 0.0000\n",
      "Avg Cons Loss: 0.0154\n",
      "Time Taken: 11.01s\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ---\n",
    "# jupyter:\n",
    "#   jupytext:\n",
    "#     text_representation:\n",
    "#       extension: .py\n",
    "#       format_name: light\n",
    "#       format_version: '1.5'\n",
    "#       jupytext_version: 1.14.5 # Or your version\n",
    "#   kernelspec:\n",
    "#     display_name: Python 3 (ipykernel) # Or your kernel name\n",
    "#     language: python\n",
    "#     name: python3\n",
    "# ---\n",
    "\n",
    "# # BraTS 2023 SSL - Mini Training Verification Notebook\n",
    "#\n",
    "# Purpose: Verify core components (data loading, transforms, dataset, model forward/backward pass)\n",
    "# using a small subset of data and limited training steps before running the full script.\n",
    "\n",
    "# ## 1. Imports\n",
    "\n",
    "# +\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from functools import partial\n",
    "\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# MONAI Imports\n",
    "from monai.config import print_config\n",
    "from monai.data import Dataset, list_data_collate, decollate_batch\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai import transforms\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Activations,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureTyped,\n",
    "    ConvertToMultiChannelBasedOnBratsClassesd,\n",
    "    NormalizeIntensityd,\n",
    "    CropForegroundd,\n",
    "    RandSpatialCropd,\n",
    "    RandFlipd,\n",
    "    RandRotate90d,\n",
    "    RandScaleIntensityd,\n",
    "    RandShiftIntensityd,\n",
    "    RandGaussianNoised,\n",
    "    RandGaussianSmoothd,\n",
    ")\n",
    "from monai.utils.enums import MetricReduction\n",
    "\n",
    "print_config()\n",
    "# -\n",
    "\n",
    "# ## 2. Configuration (Mini-Version)\n",
    "# Hardcode paths and use small values for quick testing.\n",
    "\n",
    "# +\n",
    "# --- Mini-Config ---\n",
    "DATA_DIR = '/home/users/vraja/dl/data/brats2021challenge' # Base dir containing CSV and data folder\n",
    "STRUCTURE_CSV = 'brats_dsc.csv' # Your structure CSV\n",
    "OUTPUT_DIR = './output_brats_ssl_mini_test' # Temporary output for this test\n",
    "\n",
    "ROI_SIZE = (96, 96, 96) # Keep consistent with target model\n",
    "BATCH_SIZE = 1 # Small total batch size (e.g., 1 labeled, 1 unlabeled)\n",
    "LABELED_BS_RATIO = 0.5 # Ratio of labeled samples in batch\n",
    "LABELED_RATIO = 0.1 # Use only 10% of data as potentially labeled for this test\n",
    "DEBUG_SUBSET_SIZE = 10 # <<< Use only the first N samples overall for quick testing\n",
    "MAX_TRAIN_STEPS = 5 # <<< Run only N training steps total\n",
    "\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "CONSISTENCY_WEIGHT = 1.0\n",
    "SEED = 42\n",
    "NUM_WORKERS = 2 # Reduce workers for lighter load\n",
    "\n",
    "# --- End Mini-Config ---\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "# -\n",
    "\n",
    "# ## 3. Helper Classes & Functions (Copied from Training Script)\n",
    "\n",
    "# +\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = np.where(self.count > 0, self.sum / self.count, self.sum)\n",
    "\n",
    "def get_brats2023_datalists(data_dir, structure_csv_filename, labeled_ratio=0.4, debug_subset_size=None):\n",
    "    \"\"\"\n",
    "    Reads the BraTS 2023 structure CSV file and creates labeled/unlabeled data lists.\n",
    "    Added debug_subset_size parameter.\n",
    "    \"\"\"\n",
    "    structure_csv_path = os.path.join(data_dir, structure_csv_filename)\n",
    "    if not os.path.exists(structure_csv_path):\n",
    "        raise FileNotFoundError(f\"Structure CSV file not found: {structure_csv_path}\")\n",
    "\n",
    "    df = pd.read_csv(structure_csv_path)\n",
    "    training_data_root_identifier = \"ASNR-MICCAI-BraTS2023-GLI-Challenge-TrainingData\"\n",
    "    print(f\"Using structure CSV: {structure_csv_path}\")\n",
    "    print(f\"Identifying patient folders within paths containing: {training_data_root_identifier}\")\n",
    "\n",
    "    all_files = []\n",
    "    patient_dirs = df[(df['Is Directory'] == True) &\n",
    "                      (df['Path'].str.contains(training_data_root_identifier)) &\n",
    "                      (df['Path'].str.contains('BraTS-GLI-'))]['Path'].tolist()\n",
    "\n",
    "    print(f\"Found {len(patient_dirs)} potential patient directories.\")\n",
    "    if debug_subset_size is not None:\n",
    "        print(f\"--- Using subset of first {debug_subset_size} directories for testing ---\")\n",
    "        patient_dirs = patient_dirs[:debug_subset_size]\n",
    "\n",
    "    for patient_path in patient_dirs:\n",
    "        patient_folder_name = os.path.basename(patient_path)\n",
    "        image_files = [\n",
    "            os.path.join(patient_path, f\"{patient_folder_name}-t1c.nii.gz\"),\n",
    "            os.path.join(patient_path, f\"{patient_folder_name}-t1n.nii.gz\"),\n",
    "            os.path.join(patient_path, f\"{patient_folder_name}-t2f.nii.gz\"), # FLAIR\n",
    "            os.path.join(patient_path, f\"{patient_folder_name}-t2w.nii.gz\"), # T2\n",
    "        ]\n",
    "        label_file = os.path.join(patient_path, f\"{patient_folder_name}-seg.nii.gz\")\n",
    "\n",
    "        if all(os.path.exists(f) for f in image_files) and os.path.exists(label_file):\n",
    "            all_files.append({\"image\": image_files, \"label\": label_file})\n",
    "        else:\n",
    "            missing = [f for f in image_files + [label_file] if not os.path.exists(f)]\n",
    "            print(f\"Warning: Missing files for patient {patient_folder_name}, skipping. Missing: {missing}\")\n",
    "\n",
    "    if not all_files:\n",
    "        raise ValueError(\"No valid data files found for the subset.\")\n",
    "\n",
    "    np.random.shuffle(all_files)\n",
    "    num_labeled = int(len(all_files) * labeled_ratio)\n",
    "    labeled_files = all_files[:num_labeled]\n",
    "    unlabeled_files = all_files[num_labeled:]\n",
    "\n",
    "    print(f\"Total valid cases processed in subset: {len(all_files)}\")\n",
    "    print(f\"Using {len(labeled_files)} cases as labeled data.\")\n",
    "    print(f\"Using {len(unlabeled_files)} cases as unlabeled data.\")\n",
    "\n",
    "    # For mini-test, maybe skip validation split or use very few\n",
    "    # Let's skip validation split for simplicity here.\n",
    "    train_labeled_files = labeled_files\n",
    "    val_files = [] # No validation in this mini-script\n",
    "\n",
    "    return train_labeled_files, unlabeled_files, val_files # Return empty val_files\n",
    "\n",
    "# --- Transforms (Copied) ---\n",
    "train_transforms_weak = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"], image_only=False, ensure_channel_first=True),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"], dtype=torch.float32),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\", k_divisible=ROI_SIZE),\n",
    "        RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=ROI_SIZE, random_size=False),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "        RandRotate90d(keys=[\"image\", \"label\"], prob=0.1, max_k=3),\n",
    "        RandScaleIntensityd(keys=\"image\", factors=0.1, prob=0.5),\n",
    "        RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=0.5),\n",
    "    ]\n",
    ")\n",
    "train_transforms_strong = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\"], image_only=False, ensure_channel_first=True),\n",
    "        EnsureTyped(keys=[\"image\"], dtype=torch.float32),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        CropForegroundd(keys=[\"image\"], source_key=\"image\", k_divisible=ROI_SIZE),\n",
    "        RandSpatialCropd(keys=[\"image\"], roi_size=ROI_SIZE, random_size=False),\n",
    "        RandFlipd(keys=[\"image\"], prob=0.5, spatial_axis=0),\n",
    "        RandFlipd(keys=[\"image\"], prob=0.5, spatial_axis=1),\n",
    "        RandFlipd(keys=[\"image\"], prob=0.5, spatial_axis=2),\n",
    "        RandRotate90d(keys=[\"image\"], prob=0.1, max_k=3),\n",
    "        RandScaleIntensityd(keys=\"image\", factors=0.1, prob=0.5),\n",
    "        RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=0.5),\n",
    "        RandGaussianNoised(keys=\"image\", prob=0.3, mean=0.0, std=0.1),\n",
    "        RandGaussianSmoothd(keys=\"image\", prob=0.3, sigma_x=(0.5, 1.5), sigma_y=(0.5, 1.5), sigma_z=(0.5, 1.5)),\n",
    "        RandScaleIntensityd(keys=\"image\", factors=0.2, prob=0.2),\n",
    "        RandShiftIntensityd(keys=\"image\", offsets=0.2, prob=0.2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Custom Dataset for SSL (Copied) ---\n",
    "class SSLDataset(Dataset):\n",
    "    def __init__(self, data, transform_weak, transform_strong):\n",
    "        super().__init__(data=data, transform=None)\n",
    "        self.transform_weak = transform_weak\n",
    "        self.transform_strong = transform_strong\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_i = self.data[index]\n",
    "        data_weak = self.transform_weak(data_i.copy())\n",
    "        img_weak = data_weak['image']\n",
    "        label_weak = data_weak.get('label', None)\n",
    "        data_strong_input = {'image': data_i['image']}\n",
    "        img_strong = self.transform_strong(data_strong_input)['image']\n",
    "        output = {\"image_weak\": img_weak, \"image_strong\": img_strong}\n",
    "        if label_weak is not None:\n",
    "            output[\"label\"] = label_weak\n",
    "        return output\n",
    "# -\n",
    "\n",
    "# ## 4. Create Datasets and DataLoaders (Mini-Version)\n",
    "\n",
    "# +\n",
    "print(\"Creating datasets and dataloaders (mini-version)...\")\n",
    "train_labeled_files, unlabeled_files, _ = get_brats2023_datalists(\n",
    "    DATA_DIR, STRUCTURE_CSV, LABELED_RATIO, debug_subset_size=DEBUG_SUBSET_SIZE\n",
    ")\n",
    "\n",
    "# Create the SSL datasets\n",
    "train_ds_labeled = SSLDataset(data=train_labeled_files, transform_weak=train_transforms_weak, transform_strong=train_transforms_strong)\n",
    "train_ds_unlabeled = SSLDataset(data=unlabeled_files, transform_weak=train_transforms_weak, transform_strong=train_transforms_strong)\n",
    "\n",
    "# Calculate batch sizes for the mini-batch\n",
    "labeled_batch_size = int(BATCH_SIZE * LABELED_BS_RATIO)\n",
    "unlabeled_batch_size = BATCH_SIZE - labeled_batch_size\n",
    "\n",
    "# Adjust if one dataset is empty or batch size is 0\n",
    "if not train_labeled_files: labeled_batch_size = 0\n",
    "if not unlabeled_files: unlabeled_batch_size = 0\n",
    "if labeled_batch_size == 0 and unlabeled_batch_size > 0: labeled_batch_size = 1; unlabeled_batch_size = max(0, BATCH_SIZE - 1)\n",
    "if unlabeled_batch_size == 0 and labeled_batch_size > 0: unlabeled_batch_size = 1; labeled_batch_size = max(0, BATCH_SIZE - 1)\n",
    "\n",
    "print(f\"Mini Batch size: {labeled_batch_size + unlabeled_batch_size} (Labeled: {labeled_batch_size}, Unlabeled: {unlabeled_batch_size})\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader_labeled = DataLoader(\n",
    "    train_ds_labeled, batch_size=labeled_batch_size, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available(), collate_fn=list_data_collate, drop_last=True\n",
    ") if labeled_batch_size > 0 and train_ds_labeled else None\n",
    "\n",
    "train_loader_unlabeled = DataLoader(\n",
    "    train_ds_unlabeled, batch_size=unlabeled_batch_size, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available(), collate_fn=list_data_collate, drop_last=True\n",
    ") if unlabeled_batch_size > 0 and train_ds_unlabeled else None\n",
    "\n",
    "if not train_loader_labeled and not train_loader_unlabeled:\n",
    "     raise ValueError(\"No data loaders could be created. Check data subset and batch sizes.\")\n",
    "# -\n",
    "\n",
    "# ## 5. Initialize Model, Loss, Optimizer\n",
    "\n",
    "# +\n",
    "print(\"Initializing model, loss, optimizer...\")\n",
    "model = SwinUNETR(\n",
    "    img_size=ROI_SIZE,\n",
    "    in_channels=4,\n",
    "    out_channels=3,\n",
    "    feature_size=48, # Keep consistent\n",
    "    use_checkpoint=True, # Disable checkpointing for faster mini-test\n",
    ").to(device)\n",
    "\n",
    "# Loss Functions\n",
    "supervised_loss = DiceCELoss(to_onehot_y=False, sigmoid=True, lambda_dice=0.5, lambda_ce=0.5)\n",
    "consistency_loss = torch.nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "print(\"Model, Loss, Optimizer Initialized.\")\n",
    "# -\n",
    "\n",
    "# ## 6. Mini Training Loop\n",
    "\n",
    "# +\n",
    "print(f\"--- Starting Mini Training Verification (Max Steps: {MAX_TRAIN_STEPS}) ---\")\n",
    "model.train()\n",
    "run_loss_sup = AverageMeter()\n",
    "run_loss_cons = AverageMeter()\n",
    "run_loss_total = AverageMeter()\n",
    "start_time = time.time()\n",
    "\n",
    "# Create iterators (handle potential None loaders)\n",
    "iter_l = iter(train_loader_labeled) if train_loader_labeled else None\n",
    "iter_u = iter(train_loader_unlabeled) if train_loader_unlabeled else None\n",
    "\n",
    "steps_done = 0\n",
    "while steps_done < MAX_TRAIN_STEPS:\n",
    "    optimizer.zero_grad()\n",
    "    total_loss_batch = 0.0\n",
    "    current_labeled_bs = 0\n",
    "    current_unlabeled_bs = 0\n",
    "\n",
    "    # --- Supervised Loss ---\n",
    "    if iter_l:\n",
    "        try:\n",
    "            batch_labeled = next(iter_l)\n",
    "            images_l_weak, labels_l = batch_labeled[\"image_weak\"].to(device), batch_labeled[\"label\"].to(device)\n",
    "            current_labeled_bs = images_l_weak.size(0)\n",
    "\n",
    "            print(f\"Step {steps_done+1}: Labeled shapes - Image: {images_l_weak.shape}, Label: {labels_l.shape}\")\n",
    "\n",
    "            logits_l = model(images_l_weak)\n",
    "            loss_s = supervised_loss(logits_l, labels_l)\n",
    "            run_loss_sup.update(loss_s.item(), n=current_labeled_bs)\n",
    "            total_loss_batch += loss_s\n",
    "        except StopIteration:\n",
    "            print(\"Labeled loader finished.\")\n",
    "            iter_l = None # Stop trying labeled data\n",
    "        except Exception as e:\n",
    "            print(f\"Error in labeled batch {steps_done+1}: {e}\")\n",
    "            # Decide whether to break or continue\n",
    "            break # Stop on error during mini-test\n",
    "\n",
    "    # --- Consistency Loss ---\n",
    "    if iter_u:\n",
    "        try:\n",
    "            batch_unlabeled = next(iter_u)\n",
    "            images_u_weak, images_u_strong = batch_unlabeled[\"image_weak\"].to(device), batch_unlabeled[\"image_strong\"].to(device)\n",
    "            current_unlabeled_bs = images_u_weak.size(0)\n",
    "\n",
    "            print(f\"Step {steps_done+1}: Unlabeled shapes - Weak: {images_u_weak.shape}, Strong: {images_u_strong.shape}\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits_u_weak = model(images_u_weak)\n",
    "                pseudo_labels = torch.sigmoid(logits_u_weak.detach())\n",
    "\n",
    "            logits_u_strong = model(images_u_strong)\n",
    "            preds_strong_sig = torch.sigmoid(logits_u_strong)\n",
    "            loss_c = consistency_loss(preds_strong_sig, pseudo_labels)\n",
    "            run_loss_cons.update(loss_c.item(), n=current_unlabeled_bs)\n",
    "            total_loss_batch += CONSISTENCY_WEIGHT * loss_c\n",
    "        except StopIteration:\n",
    "            print(\"Unlabeled loader finished.\")\n",
    "            iter_u = None # Stop trying unlabeled data\n",
    "        except Exception as e:\n",
    "            print(f\"Error in unlabeled batch {steps_done+1}: {e}\")\n",
    "            break # Stop on error during mini-test\n",
    "\n",
    "    # --- Backpropagation ---\n",
    "    if isinstance(total_loss_batch, torch.Tensor) and total_loss_batch != 0:\n",
    "        print(f\"Step {steps_done+1}: Calculated Total Loss: {total_loss_batch.item():.4f}\")\n",
    "        total_loss_batch.backward()\n",
    "        optimizer.step()\n",
    "        total_bs = current_labeled_bs + current_unlabeled_bs\n",
    "        if total_bs > 0:\n",
    "            run_loss_total.update(total_loss_batch.item(), n=total_bs)\n",
    "        print(f\"Step {steps_done+1}: Backward pass and optimizer step completed.\")\n",
    "    elif iter_l is None and iter_u is None:\n",
    "         print(\"Both loaders finished. Stopping mini-train.\")\n",
    "         break # Stop if both loaders are exhausted\n",
    "    else:\n",
    "        print(f\"Step {steps_done+1}: No loss computed (likely waiting for data).\")\n",
    "\n",
    "\n",
    "    steps_done += 1\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Break if both iterators are exhausted\n",
    "    if iter_l is None and iter_u is None:\n",
    "        print(\"Both data loaders exhausted before reaching max steps.\")\n",
    "        break\n",
    "\n",
    "            \n",
    "    del total_loss_batch # Delete loss tensor explicitly\n",
    "    if 'loss_s' in locals(): del loss_s\n",
    "    if 'loss_c' in locals(): del loss_c\n",
    "    if 'logits_l' in locals(): del logits_l\n",
    "    if 'logits_u_weak' in locals(): del logits_u_weak\n",
    "    if 'logits_u_strong' in locals(): del logits_u_strong\n",
    "    if 'pseudo_labels' in locals(): del pseudo_labels\n",
    "    if 'preds_strong_sig' in locals(): del preds_strong_sig\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "print(f\"--- Mini Training Verification Finished ---\")\n",
    "print(f\"Steps Run: {steps_done}\")\n",
    "print(f\"Avg Total Loss: {run_loss_total.avg:.4f}\")\n",
    "print(f\"Avg Sup Loss: {run_loss_sup.avg:.4f}\")\n",
    "print(f\"Avg Cons Loss: {run_loss_cons.avg:.4f}\")\n",
    "print(f\"Time Taken: {(time.time() - start_time):.2f}s\")\n",
    "# -\n",
    "\n",
    "# ## 7. Verification Summary\n",
    "#\n",
    "# * If the notebook ran through the `Mini Training Loop` section without crashing (especially on CUDA errors, shape mismatches, or file not found errors), it indicates:\n",
    "#     * Data loading from `brats_dsc.csv` is likely working correctly.\n",
    "#     * The `SSLDataset` class is correctly producing dictionaries with `image_weak`, `image_strong`, and `label` (when available).\n",
    "#     * Transforms are compatible with the data and model input requirements.\n",
    "#     * The model's forward pass works for both weakly and strongly augmented data.\n",
    "#     * Loss functions can compute values based on model outputs and labels/pseudo-labels.\n",
    "#     * The backward pass and optimizer step execute.\n",
    "# * Check the printed shapes during the loop to ensure they are as expected (e.g., `(BatchSize, Channels, H, W, D)`).\n",
    "# * Non-zero loss values indicate the model is learning *something*. Very high or NaN losses might indicate instability (check learning rate, normalization, etc.).\n",
    "#\n",
    "# **If this runs successfully, you have higher confidence that the full training script (`brats_ssl_train_script`) is set up correctly.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde680f4-c94f-4550-8d81-f348a15cd409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff9955e-db7a-4dd8-8b19-86585d076390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
